{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: SAT & ACT Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first markdown cell in a notebook is a great place to provide an overview of your entire project. You will likely want to at least state your\n",
    "\n",
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"problem statement goes here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "If you want to, it's great to use relative links to direct your audience to various sections of a notebook. **HERE'S A DEMONSTRATION WITH THE CURRENT SECTION HEADERS**:\n",
    "\n",
    "### Contents:\n",
    "- [2017 Data Import & Cleaning](#Data-Import-and-Cleaning)\n",
    "- [2018 Data Import and Cleaning](#2018-Data-Import-and-Cleaning)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Data Visualization](#Visualize-the-data)\n",
    "- [Descriptive and Inferential Statistics](#Descriptive-and-Inferential-Statistics)\n",
    "- [Outside Research](#Outside-Research)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you combine your problem statement, executive summary, data dictionary, and conclusions/recommendations, you have an amazing README.md file that quickly aligns your audience to the contents of your project.** Don't forget to cite your data sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All libraries used should be added here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017 Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read In SAT & ACT  Data\n",
    "\n",
    "Read in the `sat_2017.csv` and `act_2017.csv` files and assign them to appropriately named pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "sat_2017 = pd.read_csv('../data/sat_2017.csv')\n",
    "act_2017 = pd.read_csv('../data/act_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Display Data\n",
    "\n",
    "Print the first 10 rows of each dataframe to your jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Participation</th>\n",
       "      <th>Evidence-Based Reading and Writing</th>\n",
       "      <th>Math</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>5%</td>\n",
       "      <td>593</td>\n",
       "      <td>572</td>\n",
       "      <td>1165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>38%</td>\n",
       "      <td>547</td>\n",
       "      <td>533</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>30%</td>\n",
       "      <td>563</td>\n",
       "      <td>553</td>\n",
       "      <td>1116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3%</td>\n",
       "      <td>614</td>\n",
       "      <td>594</td>\n",
       "      <td>1208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>53%</td>\n",
       "      <td>531</td>\n",
       "      <td>524</td>\n",
       "      <td>1055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>11%</td>\n",
       "      <td>606</td>\n",
       "      <td>595</td>\n",
       "      <td>1201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>100%</td>\n",
       "      <td>530</td>\n",
       "      <td>512</td>\n",
       "      <td>1041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>100%</td>\n",
       "      <td>503</td>\n",
       "      <td>492</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>100%</td>\n",
       "      <td>482</td>\n",
       "      <td>468</td>\n",
       "      <td>950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Florida</td>\n",
       "      <td>83%</td>\n",
       "      <td>520</td>\n",
       "      <td>497</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  State Participation  Evidence-Based Reading and Writing  \\\n",
       "0               Alabama            5%                                 593   \n",
       "1                Alaska           38%                                 547   \n",
       "2               Arizona           30%                                 563   \n",
       "3              Arkansas            3%                                 614   \n",
       "4            California           53%                                 531   \n",
       "5              Colorado           11%                                 606   \n",
       "6           Connecticut          100%                                 530   \n",
       "7              Delaware          100%                                 503   \n",
       "8  District of Columbia          100%                                 482   \n",
       "9               Florida           83%                                 520   \n",
       "\n",
       "   Math  Total  \n",
       "0   572   1165  \n",
       "1   533   1080  \n",
       "2   553   1116  \n",
       "3   594   1208  \n",
       "4   524   1055  \n",
       "5   595   1201  \n",
       "6   512   1041  \n",
       "7   492    996  \n",
       "8   468    950  \n",
       "9   497   1017  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code:\n",
    "\n",
    "sat_2017.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Participation</th>\n",
       "      <th>English</th>\n",
       "      <th>Math</th>\n",
       "      <th>Reading</th>\n",
       "      <th>Science</th>\n",
       "      <th>Composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>National</td>\n",
       "      <td>60%</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.7</td>\n",
       "      <td>21.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>100%</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>65%</td>\n",
       "      <td>18.7</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.4</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>62%</td>\n",
       "      <td>18.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.1</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>100%</td>\n",
       "      <td>18.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>California</td>\n",
       "      <td>31%</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.7</td>\n",
       "      <td>23.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>100%</td>\n",
       "      <td>20.1</td>\n",
       "      <td>20.3</td>\n",
       "      <td>21.2</td>\n",
       "      <td>20.9</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>31%</td>\n",
       "      <td>25.5</td>\n",
       "      <td>24.6</td>\n",
       "      <td>25.6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>18%</td>\n",
       "      <td>24.1</td>\n",
       "      <td>23.4</td>\n",
       "      <td>24.8</td>\n",
       "      <td>23.6</td>\n",
       "      <td>24.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>32%</td>\n",
       "      <td>24.4</td>\n",
       "      <td>23.5</td>\n",
       "      <td>24.9</td>\n",
       "      <td>23.5</td>\n",
       "      <td>24.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  State Participation  English  Math  Reading  Science  \\\n",
       "0              National           60%     20.3  20.7     21.4     21.0   \n",
       "1               Alabama          100%     18.9  18.4     19.7     19.4   \n",
       "2                Alaska           65%     18.7  19.8     20.4     19.9   \n",
       "3               Arizona           62%     18.6  19.8     20.1     19.8   \n",
       "4              Arkansas          100%     18.9  19.0     19.7     19.5   \n",
       "5            California           31%     22.5  22.7     23.1     22.2   \n",
       "6              Colorado          100%     20.1  20.3     21.2     20.9   \n",
       "7           Connecticut           31%     25.5  24.6     25.6     24.6   \n",
       "8              Delaware           18%     24.1  23.4     24.8     23.6   \n",
       "9  District of Columbia           32%     24.4  23.5     24.9     23.5   \n",
       "\n",
       "  Composite  \n",
       "0      21.0  \n",
       "1      19.2  \n",
       "2      19.8  \n",
       "3      19.7  \n",
       "4      19.4  \n",
       "5      22.8  \n",
       "6      20.8  \n",
       "7      25.2  \n",
       "8      24.1  \n",
       "9      24.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_2017.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Verbally Describe Data\n",
    "\n",
    "Take your time looking through the data and thoroughly describe the data in the markdown cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "sat_2017: We've got data for each state's participation, reading/verbal, math, and total scores. There's an incredibly wide range of participation that varies from state to state from 2% up to 100%. Also, I think there may be at least one error, because I highly doubt the average math score in Maryland was a 52 out of 800. Perhaps some of the participation values are errors too. Other than that, the averages seem like they could be realistic.\n",
    "\n",
    "act_2017: We've got data for each state's participation, English, math, reading, science, and composite scores. Like with the SAT scores, here there is also a wide range of participation from 8% up to 100%. Scores have low values hovering around 2/36 and going up to 25 or 26 out of 36. Also, there appears to be a typo in Wyoming's composite score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Does the data look complete? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: It looks complete in that there are no empty values. However, some of these values might be errors like I mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Are there any obvious issues with the observations?\n",
    "\n",
    "**What is the minimum *possible* value for each test/subtest? What is the maximum *possible* value?**\n",
    "\n",
    "Consider comparing any questionable values to the sources of your data:\n",
    "- [SAT](https://blog.collegevine.com/here-are-the-average-sat-scores-by-state/)\n",
    "- [ACT](https://blog.prepscholar.com/act-scores-by-state-averages-highs-and-lows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Maryland's average math score on the 2017 SAT is in fact 524, not 52.\n",
    "\n",
    "The min/max possible scores on the SAT are 0/1600 with 0/800 for each part.\n",
    "\n",
    "The min/max possible scores on the ACT are 0/36 with 0/36 for each part too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4c. Fix any errors you identified\n",
    "\n",
    "**The data is available** so there's no need to guess or calculate anything. If you didn't find any errors, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the syntax here that is used to assign the integer 524 as a value to that particular cell.\n",
    "# There's a comma inside the brackets, clearly indicating ROW, COLUMN\n",
    "\n",
    "sat_2017.loc[20, 'Math'] = 524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "524"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This syntax, unlike the one above, is used to ask what the value of a particular [row][column] is, but\n",
    "# this syntax cannot be used to reassign its value. You can only change the value using the above syntax.\n",
    "\n",
    "sat_2017.loc[20]['Math']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_2017.loc[51, \"Composite\"] = 20.2\n",
    "act_2017.loc[51, \"Composite\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_2017.loc[0, \"Composite\"] = 21.0\n",
    "act_2017.loc[0, \"Composite\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What are your data types? \n",
    "Display the data types of each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State                                 object\n",
       "Participation                         object\n",
       "Evidence-Based Reading and Writing     int64\n",
       "Math                                   int64\n",
       "Total                                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_2017.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State             object\n",
       "Participation     object\n",
       "English          float64\n",
       "Math             float64\n",
       "Reading          float64\n",
       "Science          float64\n",
       "Composite         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_2017.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you learn?\n",
    "- Do any of them seem odd?  \n",
    "- Which ones are not as they should be?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The participation type in both .csv files is listed as an object, even though it's a number. Also, the Composite series in the ACT csv should be floats, not objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Fix Incorrect Data Types\n",
    "Based on what you discovered above, use appropriate methods to re-type incorrectly typed data.\n",
    "- Define a function that will allow you to convert participation rates to an appropriate numeric type. Use `map` or `apply` to change these columns in each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "sat_2017['Participation'] = sat_2017['Participation'].map(lambda x: str(x).replace('%',''))\n",
    "act_2017['Participation'] = act_2017['Participation'].map(lambda x: str(x).replace('%',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2017['Participation'] = sat_2017['Participation'].astype(float)\n",
    "act_2017['Participation'] = act_2017['Participation'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fix any individual values preventing other columns from being the appropriate type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# already fixed act_2017 typo listed above in 4C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finish your data modifications by making sure the columns are now typed appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_2017['Composite'] = act_2017['Composite'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display the data types again to confirm they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State                                  object\n",
      "Participation                         float64\n",
      "Evidence-Based Reading and Writing      int64\n",
      "Math                                    int64\n",
      "Total                                   int64\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "State             object\n",
       "Participation    float64\n",
       "English          float64\n",
       "Math             float64\n",
       "Reading          float64\n",
       "Science          float64\n",
       "Composite        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sat_2017.dtypes)\n",
    "act_2017.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Rename Columns\n",
    "Change the names of the columns to more expressive names so that you can tell the difference the SAT columns and the ACT columns. Your solution should map all column names being changed at once (no repeated singular name-changes). **We will be combining these data with some of the data from 2018, and so you should name columns in an appropriate way**.\n",
    "\n",
    "**Guidelines**:\n",
    "- Column names should be all lowercase (you will thank yourself when you start pushing data to SQL later in the course)\n",
    "- Column names should not contain spaces (underscores will suffice--this allows for using the `df.column_name` method to access columns in addition to `df['column_name']`.\n",
    "- Column names should be unique and informative (the only feature that we actually share between dataframes is the state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\n",
    "\n",
    "sat_2017 = sat_2017.rename({'State': 'state', \"Participation\": \"'17_sat_partic\", \"Evidence-Based Reading and Writing\": \"'17_sat_language\",'Math':\"'17_sat_math\", \"Total\":\"'17_sat_total\"}, axis='columns')\n",
    "act_2017 = act_2017.rename({'State': 'state', \"Participation\": \"'17_act_partic\", \"English\": \"'17_act_english\", \"Math\": \"'17_act_math\",\"Reading\": \"'17_act_reading\", \"Science\": \"'17_act_science\", \"Composite\": \"'17_act_composite\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state',\n",
       " \"'17_act_partic\",\n",
       " \"'17_act_english\",\n",
       " \"'17_act_math\",\n",
       " \"'17_act_reading\",\n",
       " \"'17_act_science\",\n",
       " \"'17_act_composite\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = []\n",
    "for col in act_2017.columns: \n",
    "    lst.append(col)\n",
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Create a data dictionary\n",
    "\n",
    "Now that we've fixed our data, and given it appropriate names, let's create a [data dictionary](http://library.ucmerced.edu/node/10249). \n",
    "\n",
    "A data dictionary provides a quick overview of features/variables/columns, alongside data types and descriptions. The more descriptive you can be, the more useful this document is.\n",
    "\n",
    "Example of a Fictional Data Dictionary Entry: \n",
    "\n",
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|**county_pop**|*integer*|2010 census|The population of the county (units in thousands, where 2.5 represents 2500 people).| \n",
    "|**per_poverty**|*float*|2010 census|The percent of the county over the age of 18 living below the 200% of official US poverty rate (units percent to two decimal places 98.10 means 98.1%)|\n",
    "\n",
    "[Here's a quick link to a short guide for formatting markdown in Jupyter notebooks](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html).\n",
    "\n",
    "Provided is the skeleton for formatting a markdown table, with columns headers that will help you create a data dictionary to quickly summarize your data, as well as some examples. **This would be a great thing to copy and paste into your custom README for this project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Feature|Type|Dataset|Description|\n",
    "|---|---|---|---|\n",
    "|column name|int/float/object|ACT/SAT|This is an example| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Drop unnecessary rows\n",
    "\n",
    "One of our dataframes contains an extra row. Identify and remove this from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>'17_act_partic</th>\n",
       "      <th>'17_act_english</th>\n",
       "      <th>'17_act_math</th>\n",
       "      <th>'17_act_reading</th>\n",
       "      <th>'17_act_science</th>\n",
       "      <th>'17_act_composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.4</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>62.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.1</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>California</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.7</td>\n",
       "      <td>23.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>20.3</td>\n",
       "      <td>21.2</td>\n",
       "      <td>20.9</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>31.0</td>\n",
       "      <td>25.5</td>\n",
       "      <td>24.6</td>\n",
       "      <td>25.6</td>\n",
       "      <td>24.6</td>\n",
       "      <td>25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>23.4</td>\n",
       "      <td>24.8</td>\n",
       "      <td>23.6</td>\n",
       "      <td>24.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>32.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>23.5</td>\n",
       "      <td>24.9</td>\n",
       "      <td>23.5</td>\n",
       "      <td>24.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Florida</td>\n",
       "      <td>73.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>55.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.3</td>\n",
       "      <td>21.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>90.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19.2</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>38.0</td>\n",
       "      <td>21.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.1</td>\n",
       "      <td>22.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>93.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>21.6</td>\n",
       "      <td>21.3</td>\n",
       "      <td>21.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>35.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.4</td>\n",
       "      <td>23.2</td>\n",
       "      <td>22.3</td>\n",
       "      <td>22.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>67.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>21.3</td>\n",
       "      <td>22.6</td>\n",
       "      <td>22.1</td>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>73.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>21.3</td>\n",
       "      <td>22.3</td>\n",
       "      <td>21.7</td>\n",
       "      <td>21.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.1</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>18.8</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Maine</td>\n",
       "      <td>8.0</td>\n",
       "      <td>24.2</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>24.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>28.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>23.1</td>\n",
       "      <td>24.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>29.0</td>\n",
       "      <td>25.4</td>\n",
       "      <td>25.3</td>\n",
       "      <td>25.9</td>\n",
       "      <td>24.7</td>\n",
       "      <td>25.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>29.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>23.7</td>\n",
       "      <td>24.5</td>\n",
       "      <td>23.8</td>\n",
       "      <td>24.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>100.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>21.5</td>\n",
       "      <td>21.8</td>\n",
       "      <td>21.6</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.1</td>\n",
       "      <td>18.8</td>\n",
       "      <td>18.8</td>\n",
       "      <td>18.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Montana</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>84.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>20.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>21.5</td>\n",
       "      <td>21.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>100.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.1</td>\n",
       "      <td>18.2</td>\n",
       "      <td>17.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>18.0</td>\n",
       "      <td>25.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>24.9</td>\n",
       "      <td>25.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.0</td>\n",
       "      <td>23.8</td>\n",
       "      <td>23.8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>66.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>New York</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>23.9</td>\n",
       "      <td>24.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.3</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>98.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.6</td>\n",
       "      <td>20.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>75.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>21.6</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>18.8</td>\n",
       "      <td>20.1</td>\n",
       "      <td>19.6</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>40.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>21.5</td>\n",
       "      <td>22.4</td>\n",
       "      <td>21.7</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.4</td>\n",
       "      <td>23.4</td>\n",
       "      <td>24.2</td>\n",
       "      <td>23.3</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>24.7</td>\n",
       "      <td>23.4</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>100.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>19.1</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>80.0</td>\n",
       "      <td>20.7</td>\n",
       "      <td>21.5</td>\n",
       "      <td>22.3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.2</td>\n",
       "      <td>20.1</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Texas</td>\n",
       "      <td>45.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>20.7</td>\n",
       "      <td>21.1</td>\n",
       "      <td>20.9</td>\n",
       "      <td>20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Utah</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.9</td>\n",
       "      <td>20.8</td>\n",
       "      <td>20.6</td>\n",
       "      <td>20.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>29.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>23.1</td>\n",
       "      <td>24.4</td>\n",
       "      <td>23.2</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>29.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>23.3</td>\n",
       "      <td>24.6</td>\n",
       "      <td>23.5</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Washington</td>\n",
       "      <td>29.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>21.9</td>\n",
       "      <td>22.1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>69.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>21.2</td>\n",
       "      <td>20.5</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>20.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.8</td>\n",
       "      <td>20.6</td>\n",
       "      <td>20.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   state  '17_act_partic  '17_act_english  '17_act_math  \\\n",
       "1                Alabama           100.0             18.9          18.4   \n",
       "2                 Alaska            65.0             18.7          19.8   \n",
       "3                Arizona            62.0             18.6          19.8   \n",
       "4               Arkansas           100.0             18.9          19.0   \n",
       "5             California            31.0             22.5          22.7   \n",
       "6               Colorado           100.0             20.1          20.3   \n",
       "7            Connecticut            31.0             25.5          24.6   \n",
       "8               Delaware            18.0             24.1          23.4   \n",
       "9   District of Columbia            32.0             24.4          23.5   \n",
       "10               Florida            73.0             19.0          19.4   \n",
       "11               Georgia            55.0             21.0          20.9   \n",
       "12                Hawaii            90.0             17.8          19.2   \n",
       "13                 Idaho            38.0             21.9          21.8   \n",
       "14              Illinois            93.0             21.0          21.2   \n",
       "15               Indiana            35.0             22.0          22.4   \n",
       "16                  Iowa            67.0             21.2          21.3   \n",
       "17                Kansas            73.0             21.1          21.3   \n",
       "18              Kentucky           100.0             19.6          19.4   \n",
       "19             Louisiana           100.0             19.4          18.8   \n",
       "20                 Maine             8.0             24.2          24.0   \n",
       "21              Maryland            28.0             23.3          23.1   \n",
       "22         Massachusetts            29.0             25.4          25.3   \n",
       "23              Michigan            29.0             24.1          23.7   \n",
       "24             Minnesota           100.0             20.4          21.5   \n",
       "25           Mississippi           100.0             18.2          18.1   \n",
       "26              Missouri           100.0             19.8          19.9   \n",
       "27               Montana           100.0             19.0          20.2   \n",
       "28              Nebraska            84.0             20.9          20.9   \n",
       "29                Nevada           100.0             16.3          18.0   \n",
       "30         New Hampshire            18.0             25.4          25.1   \n",
       "31            New Jersey            34.0             23.8          23.8   \n",
       "32            New Mexico            66.0             18.6          19.4   \n",
       "33              New York            31.0             23.8          24.0   \n",
       "34        North Carolina           100.0             17.8          19.3   \n",
       "35          North Dakota            98.0             19.0          20.4   \n",
       "36                  Ohio            75.0             21.2          21.6   \n",
       "37              Oklahoma           100.0             18.5          18.8   \n",
       "38                Oregon            40.0             21.2          21.5   \n",
       "39          Pennsylvania            23.0             23.4          23.4   \n",
       "40          Rhode Island            21.0             24.0          23.3   \n",
       "41        South Carolina           100.0             17.5          18.6   \n",
       "42          South Dakota            80.0             20.7          21.5   \n",
       "43             Tennessee           100.0             19.5          19.2   \n",
       "44                 Texas            45.0             19.5          20.7   \n",
       "45                  Utah           100.0             19.5          19.9   \n",
       "46               Vermont            29.0             23.3          23.1   \n",
       "47              Virginia            29.0             23.5          23.3   \n",
       "48            Washington            29.0             20.9          21.9   \n",
       "49         West Virginia            69.0             20.0          19.4   \n",
       "50             Wisconsin           100.0             19.7          20.4   \n",
       "51               Wyoming           100.0             19.4          19.8   \n",
       "\n",
       "    '17_act_reading  '17_act_science  '17_act_composite  \n",
       "1              19.7             19.4               19.2  \n",
       "2              20.4             19.9               19.8  \n",
       "3              20.1             19.8               19.7  \n",
       "4              19.7             19.5               19.4  \n",
       "5              23.1             22.2               22.8  \n",
       "6              21.2             20.9               20.8  \n",
       "7              25.6             24.6               25.2  \n",
       "8              24.8             23.6               24.1  \n",
       "9              24.9             23.5               24.2  \n",
       "10             21.0             19.4               19.8  \n",
       "11             22.0             21.3               21.4  \n",
       "12             19.2             19.3               19.0  \n",
       "13             23.0             22.1               22.3  \n",
       "14             21.6             21.3               21.4  \n",
       "15             23.2             22.3               22.6  \n",
       "16             22.6             22.1               21.9  \n",
       "17             22.3             21.7               21.7  \n",
       "18             20.5             20.1               20.0  \n",
       "19             19.8             19.6               19.5  \n",
       "20             24.8             23.7               24.3  \n",
       "21             24.2              2.3               23.6  \n",
       "22             25.9             24.7               25.4  \n",
       "23             24.5             23.8               24.1  \n",
       "24             21.8             21.6               21.5  \n",
       "25             18.8             18.8               18.6  \n",
       "26             20.8             20.5               20.4  \n",
       "27             21.0             20.5               20.3  \n",
       "28             21.9             21.5               21.4  \n",
       "29             18.1             18.2               17.8  \n",
       "30             26.0             24.9               25.5  \n",
       "31             24.1             23.2               23.9  \n",
       "32             20.4             20.0               19.7  \n",
       "33             24.6             23.9               24.2  \n",
       "34             19.6             19.3               19.1  \n",
       "35             20.5             20.6               20.3  \n",
       "36             22.5             22.0               22.0  \n",
       "37             20.1             19.6               19.4  \n",
       "38             22.4             21.7               21.8  \n",
       "39             24.2             23.3               23.7  \n",
       "40             24.7             23.4               24.0  \n",
       "41             19.1             18.9               18.7  \n",
       "42             22.3             22.0               21.8  \n",
       "43             20.1             19.9               19.8  \n",
       "44             21.1             20.9               20.7  \n",
       "45             20.8             20.6               20.3  \n",
       "46             24.4             23.2               23.6  \n",
       "47             24.6             23.5               23.8  \n",
       "48             22.1             22.0               21.9  \n",
       "49             21.2             20.5               20.4  \n",
       "50             20.6             20.9               20.5  \n",
       "51             20.8             20.6               20.2  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the row in question is the \"national\" data/row for the ACT, which is not included in the SAT data.\n",
    "# If it were, then removing it wouldn't be necessary, but this is required in order to merge the dataframes.\n",
    "\n",
    "act_2017.drop(act_2017.index[0])\n",
    "# If you re-run this cell for whatever reason, be sure to put a # on the previous line so that\n",
    "# it doesn't unnecessarily get rid of another row!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Merge Dataframes\n",
    "\n",
    "Join the 2017 ACT and SAT dataframes using the state in each dataframe as the key. Assign this to a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_17 = pd.merge(sat_2017, act_2017, on ='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>'17_sat_partic</th>\n",
       "      <th>'17_sat_language</th>\n",
       "      <th>'17_sat_math</th>\n",
       "      <th>'17_sat_total</th>\n",
       "      <th>'17_act_partic</th>\n",
       "      <th>'17_act_english</th>\n",
       "      <th>'17_act_math</th>\n",
       "      <th>'17_act_reading</th>\n",
       "      <th>'17_act_science</th>\n",
       "      <th>'17_act_composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>5.0</td>\n",
       "      <td>593</td>\n",
       "      <td>572</td>\n",
       "      <td>1165</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>38.0</td>\n",
       "      <td>547</td>\n",
       "      <td>533</td>\n",
       "      <td>1080</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.4</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>30.0</td>\n",
       "      <td>563</td>\n",
       "      <td>553</td>\n",
       "      <td>1116</td>\n",
       "      <td>62.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.1</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3.0</td>\n",
       "      <td>614</td>\n",
       "      <td>594</td>\n",
       "      <td>1208</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>53.0</td>\n",
       "      <td>531</td>\n",
       "      <td>524</td>\n",
       "      <td>1055</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.7</td>\n",
       "      <td>23.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>22.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  '17_sat_partic  '17_sat_language  '17_sat_math  '17_sat_total  \\\n",
       "0     Alabama             5.0               593           572           1165   \n",
       "1      Alaska            38.0               547           533           1080   \n",
       "2     Arizona            30.0               563           553           1116   \n",
       "3    Arkansas             3.0               614           594           1208   \n",
       "4  California            53.0               531           524           1055   \n",
       "\n",
       "   '17_act_partic  '17_act_english  '17_act_math  '17_act_reading  \\\n",
       "0           100.0             18.9          18.4             19.7   \n",
       "1            65.0             18.7          19.8             20.4   \n",
       "2            62.0             18.6          19.8             20.1   \n",
       "3           100.0             18.9          19.0             19.7   \n",
       "4            31.0             22.5          22.7             23.1   \n",
       "\n",
       "   '17_act_science  '17_act_composite  \n",
       "0             19.4               19.2  \n",
       "1             19.9               19.8  \n",
       "2             19.8               19.7  \n",
       "3             19.5               19.4  \n",
       "4             22.2               22.8  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_17.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Save your cleaned, merged dataframe\n",
    "\n",
    "Use a relative path to save out your data as `combined_2017.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_17.to_csv('combined_17.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018 Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links to the 2018 ACT and SAT data are provided in the README. These data live in PDFs, and so you'll get to enjoy practicing some *manual* data collection. Save these data as a CSV in your `data` directory, and import, explore, and clean these data in the same way you did above. **Make sure you comment on your steps so it is clear *why* you are doing each process**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2018 = pd.read_csv('../data/sat_2018.csv')\n",
    "act_2018 = pd.read_csv('../data/act_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2018['Participation'] = sat_2018['Participation'].map(lambda x: str(x).replace('%',''))\n",
    "act_2018['Participation'] = act_2018['Participation'].map(lambda x: str(x).replace('%',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2018['Participation'] = sat_2018['Participation'].astype(float)\n",
    "act_2018['Participation'] = act_2018['Participation'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_2018 = sat_2018.rename({'State': 'state', 'Participation': \"'18_sat_partic\", \"Evidence-Based Reading and Writing\": \"'18_sat_language\",'Math':\"'18_sat_math\", \"Total\":\"'18_sat_total\"}, axis='columns')\n",
    "act_2018 = act_2018.rename({'State': 'state', \"Participation\": \"'18_act_partic\", \"Composite\": \"'18_act_composite\"}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state', \"'18_act_partic\", \"'18_act_composite\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst3 = []\n",
    "for col in act_2018.columns: \n",
    "    lst3.append(col)\n",
    "lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>'18_sat_partic</th>\n",
       "      <th>'18_sat_language</th>\n",
       "      <th>'18_sat_math</th>\n",
       "      <th>'18_sat_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>6.0</td>\n",
       "      <td>595</td>\n",
       "      <td>571</td>\n",
       "      <td>1166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>43.0</td>\n",
       "      <td>562</td>\n",
       "      <td>544</td>\n",
       "      <td>1106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>29.0</td>\n",
       "      <td>577</td>\n",
       "      <td>572</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>5.0</td>\n",
       "      <td>592</td>\n",
       "      <td>576</td>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>60.0</td>\n",
       "      <td>540</td>\n",
       "      <td>536</td>\n",
       "      <td>1076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  '18_sat_partic  '18_sat_language  '18_sat_math  '18_sat_total\n",
       "0     Alabama             6.0               595           571           1166\n",
       "1      Alaska            43.0               562           544           1106\n",
       "2     Arizona            29.0               577           572           1149\n",
       "3    Arkansas             5.0               592           576           1169\n",
       "4  California            60.0               540           536           1076"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>'18_act_partic</th>\n",
       "      <th>'18_act_composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>66.0</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>District of columbia</td>\n",
       "      <td>32.0</td>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Florida</td>\n",
       "      <td>66.0</td>\n",
       "      <td>19.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  state  '18_act_partic  '18_act_composite\n",
       "0               Alabama           100.0               19.1\n",
       "1                Alaska            33.0               20.8\n",
       "2               Arizona            66.0               19.2\n",
       "3              Arkansas           100.0               19.4\n",
       "4            California            27.0               22.7\n",
       "5              Colorado            30.0               23.9\n",
       "6           Connecticut            26.0               25.6\n",
       "7              Delaware            17.0               23.8\n",
       "8  District of columbia            32.0               23.6\n",
       "9               Florida            66.0               19.9"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_2018.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "act_2018['state'][8] = 'District of Columbia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'District of Columbia'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_2018['state'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_18 = pd.merge(sat_2018, act_2018, on ='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_18.to_csv('combined_18.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine your 2017 and 2018 data into a single dataframe\n",
    "Joining on state names should work, assuming you formatted all your state names identically. Make sure none of your columns (other than state) have identical names. Do yourself a favor and decide if you're encoding participation rates as floats or integers and standardize this across your datasets.\n",
    "\n",
    "Save the contents of this merged dataframe as `final.csv`.\n",
    "\n",
    "**Use this combined dataframe for the remainder of the project**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all = pd.merge(combined_17, combined_18, on ='state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "\n",
    "### Summary Statistics\n",
    "Transpose the output of pandas `describe` method to create a quick overview of each numeric feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>'17_sat_partic</th>\n",
       "      <th>'17_sat_language</th>\n",
       "      <th>'17_sat_math</th>\n",
       "      <th>'17_sat_total</th>\n",
       "      <th>'17_act_partic</th>\n",
       "      <th>'17_act_english</th>\n",
       "      <th>'17_act_math</th>\n",
       "      <th>'17_act_reading</th>\n",
       "      <th>'17_act_science</th>\n",
       "      <th>'17_act_composite</th>\n",
       "      <th>'18_sat_partic</th>\n",
       "      <th>'18_sat_language</th>\n",
       "      <th>'18_sat_math</th>\n",
       "      <th>'18_sat_total</th>\n",
       "      <th>'18_act_partic</th>\n",
       "      <th>'18_act_composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>5.0</td>\n",
       "      <td>593</td>\n",
       "      <td>572</td>\n",
       "      <td>1165</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.4</td>\n",
       "      <td>19.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>595</td>\n",
       "      <td>571</td>\n",
       "      <td>1166</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>38.0</td>\n",
       "      <td>547</td>\n",
       "      <td>533</td>\n",
       "      <td>1080</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.4</td>\n",
       "      <td>19.9</td>\n",
       "      <td>19.8</td>\n",
       "      <td>43.0</td>\n",
       "      <td>562</td>\n",
       "      <td>544</td>\n",
       "      <td>1106</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>30.0</td>\n",
       "      <td>563</td>\n",
       "      <td>553</td>\n",
       "      <td>1116</td>\n",
       "      <td>62.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.1</td>\n",
       "      <td>19.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>29.0</td>\n",
       "      <td>577</td>\n",
       "      <td>572</td>\n",
       "      <td>1149</td>\n",
       "      <td>66.0</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3.0</td>\n",
       "      <td>614</td>\n",
       "      <td>594</td>\n",
       "      <td>1208</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.5</td>\n",
       "      <td>19.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>592</td>\n",
       "      <td>576</td>\n",
       "      <td>1169</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>53.0</td>\n",
       "      <td>531</td>\n",
       "      <td>524</td>\n",
       "      <td>1055</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.7</td>\n",
       "      <td>23.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>540</td>\n",
       "      <td>536</td>\n",
       "      <td>1076</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  '17_sat_partic  '17_sat_language  '17_sat_math  '17_sat_total  \\\n",
       "0     Alabama             5.0               593           572           1165   \n",
       "1      Alaska            38.0               547           533           1080   \n",
       "2     Arizona            30.0               563           553           1116   \n",
       "3    Arkansas             3.0               614           594           1208   \n",
       "4  California            53.0               531           524           1055   \n",
       "\n",
       "   '17_act_partic  '17_act_english  '17_act_math  '17_act_reading  \\\n",
       "0           100.0             18.9          18.4             19.7   \n",
       "1            65.0             18.7          19.8             20.4   \n",
       "2            62.0             18.6          19.8             20.1   \n",
       "3           100.0             18.9          19.0             19.7   \n",
       "4            31.0             22.5          22.7             23.1   \n",
       "\n",
       "   '17_act_science  '17_act_composite  '18_sat_partic  '18_sat_language  \\\n",
       "0             19.4               19.2             6.0               595   \n",
       "1             19.9               19.8            43.0               562   \n",
       "2             19.8               19.7            29.0               577   \n",
       "3             19.5               19.4             5.0               592   \n",
       "4             22.2               22.8            60.0               540   \n",
       "\n",
       "   '18_sat_math  '18_sat_total  '18_act_partic  '18_act_composite  \n",
       "0           571           1166           100.0               19.1  \n",
       "1           544           1106            33.0               20.8  \n",
       "2           572           1149            66.0               19.2  \n",
       "3           576           1169           100.0               19.4  \n",
       "4           536           1076            27.0               22.7  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code:\n",
    "combined_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>'17_sat_partic</th>\n",
       "      <th>'17_sat_language</th>\n",
       "      <th>'17_sat_math</th>\n",
       "      <th>'17_sat_total</th>\n",
       "      <th>'17_act_partic</th>\n",
       "      <th>'17_act_english</th>\n",
       "      <th>'17_act_math</th>\n",
       "      <th>'17_act_reading</th>\n",
       "      <th>'17_act_science</th>\n",
       "      <th>'17_act_composite</th>\n",
       "      <th>'18_sat_partic</th>\n",
       "      <th>'18_sat_language</th>\n",
       "      <th>'18_sat_math</th>\n",
       "      <th>'18_sat_total</th>\n",
       "      <th>'18_act_partic</th>\n",
       "      <th>'18_act_composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>40.865385</td>\n",
       "      <td>568.038462</td>\n",
       "      <td>555.769231</td>\n",
       "      <td>1123.903846</td>\n",
       "      <td>64.153846</td>\n",
       "      <td>20.994231</td>\n",
       "      <td>21.236538</td>\n",
       "      <td>22.067308</td>\n",
       "      <td>21.092308</td>\n",
       "      <td>21.573077</td>\n",
       "      <td>46.769231</td>\n",
       "      <td>562.692308</td>\n",
       "      <td>555.173077</td>\n",
       "      <td>1117.961538</td>\n",
       "      <td>60.673077</td>\n",
       "      <td>21.544231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>35.757916</td>\n",
       "      <td>45.881758</td>\n",
       "      <td>47.342563</td>\n",
       "      <td>92.940263</td>\n",
       "      <td>32.799680</td>\n",
       "      <td>2.374159</td>\n",
       "      <td>2.000983</td>\n",
       "      <td>2.083052</td>\n",
       "      <td>3.172606</td>\n",
       "      <td>2.037599</td>\n",
       "      <td>37.677483</td>\n",
       "      <td>47.577610</td>\n",
       "      <td>47.918117</td>\n",
       "      <td>94.401305</td>\n",
       "      <td>34.545634</td>\n",
       "      <td>2.119417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>482.000000</td>\n",
       "      <td>468.000000</td>\n",
       "      <td>950.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>977.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>17.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>531.750000</td>\n",
       "      <td>522.500000</td>\n",
       "      <td>1054.250000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>20.475000</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>19.800000</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>530.500000</td>\n",
       "      <td>521.750000</td>\n",
       "      <td>1047.750000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>19.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>40.500000</td>\n",
       "      <td>558.500000</td>\n",
       "      <td>544.500000</td>\n",
       "      <td>1104.500000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>20.800000</td>\n",
       "      <td>21.050000</td>\n",
       "      <td>21.850000</td>\n",
       "      <td>21.300000</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>543.500000</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>21.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>67.500000</td>\n",
       "      <td>612.500000</td>\n",
       "      <td>597.000000</td>\n",
       "      <td>1210.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>23.325000</td>\n",
       "      <td>23.150000</td>\n",
       "      <td>24.200000</td>\n",
       "      <td>23.200000</td>\n",
       "      <td>23.625000</td>\n",
       "      <td>79.250000</td>\n",
       "      <td>608.250000</td>\n",
       "      <td>592.750000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>23.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>644.000000</td>\n",
       "      <td>651.000000</td>\n",
       "      <td>1295.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>25.300000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>24.900000</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>643.000000</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>1298.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       '17_sat_partic  '17_sat_language  '17_sat_math  '17_sat_total  \\\n",
       "count       52.000000         52.000000     52.000000      52.000000   \n",
       "mean        40.865385        568.038462    555.769231    1123.903846   \n",
       "std         35.757916         45.881758     47.342563      92.940263   \n",
       "min          2.000000        482.000000    468.000000     950.000000   \n",
       "25%          4.000000        531.750000    522.500000    1054.250000   \n",
       "50%         40.500000        558.500000    544.500000    1104.500000   \n",
       "75%         67.500000        612.500000    597.000000    1210.000000   \n",
       "max        100.000000        644.000000    651.000000    1295.000000   \n",
       "\n",
       "       '17_act_partic  '17_act_english  '17_act_math  '17_act_reading  \\\n",
       "count       52.000000        52.000000     52.000000        52.000000   \n",
       "mean        64.153846        20.994231     21.236538        22.067308   \n",
       "std         32.799680         2.374159      2.000983         2.083052   \n",
       "min          8.000000        16.300000     18.000000        18.100000   \n",
       "25%         31.000000        19.000000     19.400000        20.475000   \n",
       "50%         68.000000        20.800000     21.050000        21.850000   \n",
       "75%        100.000000        23.325000     23.150000        24.200000   \n",
       "max        100.000000        25.500000     25.300000        26.000000   \n",
       "\n",
       "       '17_act_science  '17_act_composite  '18_sat_partic  '18_sat_language  \\\n",
       "count        52.000000          52.000000       52.000000         52.000000   \n",
       "mean         21.092308          21.573077       46.769231        562.692308   \n",
       "std           3.172606           2.037599       37.677483         47.577610   \n",
       "min           2.300000          17.800000        2.000000        480.000000   \n",
       "25%          19.900000          19.800000        4.750000        530.500000   \n",
       "50%          21.300000          21.400000       53.500000        551.000000   \n",
       "75%          23.200000          23.625000       79.250000        608.250000   \n",
       "max          24.900000          25.500000      100.000000        643.000000   \n",
       "\n",
       "       '18_sat_math  '18_sat_total  '18_act_partic  '18_act_composite  \n",
       "count     52.000000      52.000000       52.000000          52.000000  \n",
       "mean     555.173077    1117.961538       60.673077          21.544231  \n",
       "std       47.918117      94.401305       34.545634           2.119417  \n",
       "min      480.000000     977.000000        7.000000          17.700000  \n",
       "25%      521.750000    1047.750000       27.000000          19.975000  \n",
       "50%      543.500000    1096.000000       65.500000          21.300000  \n",
       "75%      592.750000    1201.000000      100.000000          23.725000  \n",
       "max      655.000000    1298.000000      100.000000          25.600000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually calculate standard deviation\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2}$$\n",
    "\n",
    "- Write a function to calculate standard deviation using the formula above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.707825127659933"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stand_dev(lst):\n",
    "    avg = sum(lst) / len(lst)\n",
    "    new_list = []\n",
    "    for i in lst:\n",
    "        everything_else = (avg - i) ** 2\n",
    "        new_list.append(everything_else)\n",
    "    variance = sum(new_list) / len(lst)\n",
    "    answer = variance ** 0.5\n",
    "    return answer\n",
    "\n",
    "list5 = [1, 2, 3, 4, 5, 6]\n",
    "stand_dev(list5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.707825127659933"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(list5)\n",
    "\n",
    "# numbers match, which mean my above formula is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use a **dictionary comprehension** to apply your standard deviation function to each numeric column in the dataframe.  **No loops**  \n",
    "- Assign the output to variable `sd` as a dictionary where: \n",
    "    - Each column name is now a key \n",
    "    - That standard deviation of the column is the value \n",
    "\n",
    "*Example Output :* `{'ACT_Math': 120, 'ACT_Reading': 120, ...}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'17_sat_partic</th>\n",
       "      <td>35.412421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'17_sat_language</th>\n",
       "      <td>45.438446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'17_sat_math</th>\n",
       "      <td>46.885136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'17_sat_total</th>\n",
       "      <td>92.042269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'17_act_partic</th>\n",
       "      <td>32.482767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'17_act_english</th>\n",
       "      <td>2.351220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'17_act_math</th>\n",
       "      <td>1.981649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'17_act_reading</th>\n",
       "      <td>2.062926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'17_act_science</th>\n",
       "      <td>3.141952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'17_act_composite</th>\n",
       "      <td>2.017912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'18_sat_partic</th>\n",
       "      <td>37.313441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'18_sat_language</th>\n",
       "      <td>47.117912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'18_sat_math</th>\n",
       "      <td>47.455129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'18_sat_total</th>\n",
       "      <td>93.489193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'18_act_partic</th>\n",
       "      <td>34.211852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'18_act_composite</th>\n",
       "      <td>2.098939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "'17_sat_partic     35.412421\n",
       "'17_sat_language   45.438446\n",
       "'17_sat_math       46.885136\n",
       "'17_sat_total      92.042269\n",
       "'17_act_partic     32.482767\n",
       "'17_act_english     2.351220\n",
       "'17_act_math        1.981649\n",
       "'17_act_reading     2.062926\n",
       "'17_act_science     3.141952\n",
       "'17_act_composite   2.017912\n",
       "'18_sat_partic     37.313441\n",
       "'18_sat_language   47.117912\n",
       "'18_sat_math       47.455129\n",
       "'18_sat_total      93.489193\n",
       "'18_act_partic     34.211852\n",
       "'18_act_composite   2.098939"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_numericals = combined_all.drop(['state'], axis=1)\n",
    "\n",
    "df_numericals = pd.DataFrame(df_numericals.apply(stand_dev, axis=0))\n",
    "df_numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'17_sat_partic\": 35.412421140129865,\n",
       " \"'17_sat_language\": 45.43844591162758,\n",
       " \"'17_sat_math\": 46.8851359849938,\n",
       " \"'17_sat_total\": 92.0422686377098,\n",
       " \"'17_act_partic\": 32.482767393108496,\n",
       " \"'17_act_english\": 2.351220097469729,\n",
       " \"'17_act_math\": 1.9816493563689903,\n",
       " \"'17_act_reading\": 2.06292558815109,\n",
       " \"'17_act_science\": 3.1419522442783,\n",
       " \"'17_act_composite\": 2.017911657040839,\n",
       " \"'18_sat_partic\": 37.3134412699805,\n",
       " \"'18_sat_language\": 47.11791202012345,\n",
       " \"'18_sat_math\": 47.455129071028026,\n",
       " \"'18_sat_total\": 93.48919343935334,\n",
       " \"'18_act_partic\": 34.21185194650338,\n",
       " \"'18_act_composite\": 2.098938693495657}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = dict(zip(df_numericals.index, df_numericals[0]))\n",
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do your manually calculated standard deviations match up with the output from pandas `describe`? What about numpy's `std` method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.412421140129865"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(combined_all[\"'17_sat_partic\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost. My manually calculated standard deviations match up with the output from Pandas' `describe`, but numpy's `std` method gives standard deviations whose decimal values are slightly different. I suppose numpy's formula for calculating SD must be very slightly different. Based on my knowledge of math/statistics, it could be in the denominators (\"n\" vs. \"n-1\"), otherwise I'm not sure what else it would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all['sat_participation_net_change'] = combined_all[\"'18_sat_partic\"] - combined_all[\"'17_sat_partic\"]\n",
    "combined_all['sat_reading_net_change'] = combined_all[\"'18_sat_language\"] - combined_all[\"'17_sat_language\"]\n",
    "combined_all['sat_math_net_change'] = combined_all[\"'18_sat_math\"] - combined_all[\"'17_sat_math\"]\n",
    "combined_all['sat_total_net_change'] = combined_all[\"'18_sat_total\"] - combined_all[\"'17_sat_total\"]\n",
    "combined_all['act_participation_net_change'] = combined_all[\"'18_act_partic\"] - combined_all[\"'17_act_partic\"]\n",
    "combined_all['act_composite_net_change'] = combined_all[\"'18_act_composite\"] - combined_all[\"'17_act_composite\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>'17_sat_partic</th>\n",
       "      <th>'17_sat_language</th>\n",
       "      <th>'17_sat_math</th>\n",
       "      <th>'17_sat_total</th>\n",
       "      <th>'17_act_partic</th>\n",
       "      <th>'17_act_english</th>\n",
       "      <th>'17_act_math</th>\n",
       "      <th>'17_act_reading</th>\n",
       "      <th>'17_act_science</th>\n",
       "      <th>...</th>\n",
       "      <th>'18_sat_math</th>\n",
       "      <th>'18_sat_total</th>\n",
       "      <th>'18_act_partic</th>\n",
       "      <th>'18_act_composite</th>\n",
       "      <th>sat_participation_net_change</th>\n",
       "      <th>sat_reading_net_change</th>\n",
       "      <th>sat_math_net_change</th>\n",
       "      <th>sat_total_net_change</th>\n",
       "      <th>act_participation_net_change</th>\n",
       "      <th>act_composite_net_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>5.0</td>\n",
       "      <td>593</td>\n",
       "      <td>572</td>\n",
       "      <td>1165</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.4</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.4</td>\n",
       "      <td>...</td>\n",
       "      <td>571</td>\n",
       "      <td>1166</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>38.0</td>\n",
       "      <td>547</td>\n",
       "      <td>533</td>\n",
       "      <td>1080</td>\n",
       "      <td>65.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.4</td>\n",
       "      <td>19.9</td>\n",
       "      <td>...</td>\n",
       "      <td>544</td>\n",
       "      <td>1106</td>\n",
       "      <td>33.0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>30.0</td>\n",
       "      <td>563</td>\n",
       "      <td>553</td>\n",
       "      <td>1116</td>\n",
       "      <td>62.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>19.8</td>\n",
       "      <td>20.1</td>\n",
       "      <td>19.8</td>\n",
       "      <td>...</td>\n",
       "      <td>572</td>\n",
       "      <td>1149</td>\n",
       "      <td>66.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>3.0</td>\n",
       "      <td>614</td>\n",
       "      <td>594</td>\n",
       "      <td>1208</td>\n",
       "      <td>100.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>19.5</td>\n",
       "      <td>...</td>\n",
       "      <td>576</td>\n",
       "      <td>1169</td>\n",
       "      <td>100.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-22</td>\n",
       "      <td>-18</td>\n",
       "      <td>-39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>53.0</td>\n",
       "      <td>531</td>\n",
       "      <td>524</td>\n",
       "      <td>1055</td>\n",
       "      <td>31.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>22.7</td>\n",
       "      <td>23.1</td>\n",
       "      <td>22.2</td>\n",
       "      <td>...</td>\n",
       "      <td>536</td>\n",
       "      <td>1076</td>\n",
       "      <td>27.0</td>\n",
       "      <td>22.7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  '17_sat_partic  '17_sat_language  '17_sat_math  '17_sat_total  \\\n",
       "0     Alabama             5.0               593           572           1165   \n",
       "1      Alaska            38.0               547           533           1080   \n",
       "2     Arizona            30.0               563           553           1116   \n",
       "3    Arkansas             3.0               614           594           1208   \n",
       "4  California            53.0               531           524           1055   \n",
       "\n",
       "   '17_act_partic  '17_act_english  '17_act_math  '17_act_reading  \\\n",
       "0           100.0             18.9          18.4             19.7   \n",
       "1            65.0             18.7          19.8             20.4   \n",
       "2            62.0             18.6          19.8             20.1   \n",
       "3           100.0             18.9          19.0             19.7   \n",
       "4            31.0             22.5          22.7             23.1   \n",
       "\n",
       "   '17_act_science  ...  '18_sat_math  '18_sat_total  '18_act_partic  \\\n",
       "0             19.4  ...           571           1166           100.0   \n",
       "1             19.9  ...           544           1106            33.0   \n",
       "2             19.8  ...           572           1149            66.0   \n",
       "3             19.5  ...           576           1169           100.0   \n",
       "4             22.2  ...           536           1076            27.0   \n",
       "\n",
       "   '18_act_composite  sat_participation_net_change  sat_reading_net_change  \\\n",
       "0               19.1                           1.0                       2   \n",
       "1               20.8                           5.0                      15   \n",
       "2               19.2                          -1.0                      14   \n",
       "3               19.4                           2.0                     -22   \n",
       "4               22.7                           7.0                       9   \n",
       "\n",
       "   sat_math_net_change  sat_total_net_change  act_participation_net_change  \\\n",
       "0                   -1                     1                           0.0   \n",
       "1                   11                    26                         -32.0   \n",
       "2                   19                    33                           4.0   \n",
       "3                  -18                   -39                           0.0   \n",
       "4                   12                    21                          -4.0   \n",
       "\n",
       "   act_composite_net_change  \n",
       "0                      -0.1  \n",
       "1                       1.0  \n",
       "2                      -0.5  \n",
       "3                       0.0  \n",
       "4                      -0.1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combined_all.to_csv('combined_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means:\n",
      "\n",
      "'17_sat_partic                    40.865385\n",
      "'17_sat_language                 568.038462\n",
      "'17_sat_math                     555.769231\n",
      "'17_sat_total                   1123.903846\n",
      "'17_act_partic                    64.153846\n",
      "'17_act_english                   20.994231\n",
      "'17_act_math                      21.236538\n",
      "'17_act_reading                   22.067308\n",
      "'17_act_science                   21.092308\n",
      "'17_act_composite                 21.573077\n",
      "'18_sat_partic                    46.769231\n",
      "'18_sat_language                 562.692308\n",
      "'18_sat_math                     555.173077\n",
      "'18_sat_total                   1117.961538\n",
      "'18_act_partic                    60.673077\n",
      "'18_act_composite                 21.544231\n",
      "sat_participation_net_change       5.903846\n",
      "sat_reading_net_change            -5.346154\n",
      "sat_math_net_change               -0.596154\n",
      "sat_total_net_change              -5.942308\n",
      "act_participation_net_change      -3.480769\n",
      "act_composite_net_change          -0.028846\n",
      "dtype: float64\n",
      "\n",
      "Medians:\n",
      "\n",
      "'17_sat_partic                    40.50\n",
      "'17_sat_language                 558.50\n",
      "'17_sat_math                     544.50\n",
      "'17_sat_total                   1104.50\n",
      "'17_act_partic                    68.00\n",
      "'17_act_english                   20.80\n",
      "'17_act_math                      21.05\n",
      "'17_act_reading                   21.85\n",
      "'17_act_science                   21.30\n",
      "'17_act_composite                 21.40\n",
      "'18_sat_partic                    53.50\n",
      "'18_sat_language                 551.00\n",
      "'18_sat_math                     543.50\n",
      "'18_sat_total                   1096.00\n",
      "'18_act_partic                    65.50\n",
      "'18_act_composite                 21.30\n",
      "sat_participation_net_change       1.00\n",
      "sat_reading_net_change             2.00\n",
      "sat_math_net_change                4.00\n",
      "sat_total_net_change               6.00\n",
      "act_participation_net_change      -1.00\n",
      "act_composite_net_change          -0.10\n",
      "dtype: float64\n",
      "\n",
      "Maximums:\n",
      "\n",
      "state                           Wyoming\n",
      "'17_sat_partic                    100.0\n",
      "'17_sat_language                    644\n",
      "'17_sat_math                        651\n",
      "'17_sat_total                      1295\n",
      "'17_act_partic                    100.0\n",
      "'17_act_english                    25.5\n",
      "'17_act_math                       25.3\n",
      "'17_act_reading                    26.0\n",
      "'17_act_science                    24.9\n",
      "'17_act_composite                  25.5\n",
      "'18_sat_partic                    100.0\n",
      "'18_sat_language                    643\n",
      "'18_sat_math                        655\n",
      "'18_sat_total                      1298\n",
      "'18_act_partic                    100.0\n",
      "'18_act_composite                  25.6\n",
      "sat_participation_net_change       90.0\n",
      "sat_reading_net_change               30\n",
      "sat_math_net_change                  52\n",
      "sat_total_net_change                 82\n",
      "act_participation_net_change       25.0\n",
      "act_composite_net_change            3.1\n",
      "dtype: object\n",
      "\n",
      "Minimums:\n",
      "\n",
      "state                           Alabama\n",
      "'17_sat_partic                      2.0\n",
      "'17_sat_language                    482\n",
      "'17_sat_math                        468\n",
      "'17_sat_total                       950\n",
      "'17_act_partic                      8.0\n",
      "'17_act_english                    16.3\n",
      "'17_act_math                       18.0\n",
      "'17_act_reading                    18.1\n",
      "'17_act_science                     2.3\n",
      "'17_act_composite                  17.8\n",
      "'18_sat_partic                      2.0\n",
      "'18_sat_language                    480\n",
      "'18_sat_math                        480\n",
      "'18_sat_total                       977\n",
      "'18_act_partic                      7.0\n",
      "'18_act_composite                  17.7\n",
      "sat_participation_net_change      -27.0\n",
      "sat_reading_net_change             -144\n",
      "sat_math_net_change                 -89\n",
      "sat_total_net_change               -228\n",
      "act_participation_net_change      -70.0\n",
      "act_composite_net_change           -1.7\n",
      "dtype: object\n",
      "\n",
      "Standard Deviations:\n",
      "\n",
      "'17_sat_partic                  35.757916\n",
      "'17_sat_language                45.881758\n",
      "'17_sat_math                    47.342563\n",
      "'17_sat_total                   92.940263\n",
      "'17_act_partic                  32.799680\n",
      "'17_act_english                  2.374159\n",
      "'17_act_math                     2.000983\n",
      "'17_act_reading                  2.083052\n",
      "'17_act_science                  3.172606\n",
      "'17_act_composite                2.037599\n",
      "'18_sat_partic                  37.677483\n",
      "'18_sat_language                47.577610\n",
      "'18_sat_math                    47.918117\n",
      "'18_sat_total                   94.401305\n",
      "'18_act_partic                  34.545634\n",
      "'18_act_composite                2.119417\n",
      "sat_participation_net_change    18.093721\n",
      "sat_reading_net_change          28.185579\n",
      "sat_math_net_change             23.123928\n",
      "sat_total_net_change            50.184918\n",
      "act_participation_net_change    13.329152\n",
      "act_composite_net_change         0.707200\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Means:\")\n",
    "print()\n",
    "print(combined_all.mean())\n",
    "print()\n",
    "print(\"Medians:\")\n",
    "print()\n",
    "print(combined_all.median())\n",
    "print()\n",
    "print(\"Maximums:\")\n",
    "print()\n",
    "print(combined_all.max())\n",
    "print()\n",
    "print(\"Minimums:\")\n",
    "print()\n",
    "print(combined_all.min())\n",
    "print()\n",
    "print(\"Standard Deviations:\")\n",
    "print()\n",
    "print(combined_all.std())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate trends in the data\n",
    "Using sorting and/or masking (along with the `.head` method to not print our entire dataframe), consider the following questions:\n",
    "\n",
    "- Which states have the highest and lowest participation rates for the:\n",
    "    - 2017 SAT?\n",
    "    - 2018 SAT?\n",
    "    - 2017 ACT?\n",
    "    - 2018 ACT?\n",
    "- Which states have the highest and lowest mean total/composite scores for the:\n",
    "    - 2017 SAT?\n",
    "    - 2018 SAT?\n",
    "    - 2017 ACT?\n",
    "    - 2018 ACT?\n",
    "- Do any states with 100% participation on a given test have a rate change year-to-year?\n",
    "- Do any states show have >50% participation on *both* tests either year?\n",
    "\n",
    "Based on what you've just observed, have you identified any states that you're especially interested in? **Make a note of these and state *why* you think they're interesting**.\n",
    "\n",
    "**You should comment on your findings at each step in a markdown cell below your code block**. Make sure you include at least one example of sorting your dataframe by a column, and one example of using boolean filtering (i.e., masking) to select a subset of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data\n",
    "\n",
    "There's not a magic bullet recommendation for the right number of plots to understand a given dataset, but visualizing your data is *always* a good idea. Not only does it allow you to quickly convey your findings (even if you have a non-technical audience), it will often reveal trends in your data that escaped you when you were looking only at numbers.\n",
    "\n",
    "Some recommendations on plotting:\n",
    "- Plots have titles\n",
    "- Plots have axis labels\n",
    "- Plots have appropriate tick labels\n",
    "- All text is legible in a plot\n",
    "- Plots demonstrate meaningful and valid relationships\n",
    "- Plots are interpreted to aid understanding\n",
    "\n",
    "There is such a thing as too many plots, and there are a *lot* of bad plots. You might make some! (But hopefully not with the guided prompts below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Seaborn's heatmap with pandas `.corr()` to visualize correlations between all numeric features\n",
    "\n",
    "Heatmaps are generally not appropriate for presentations, and should often be excluded from reports as they can be visually overwhelming. **However**, they can be extremely useful in identify relationships of potential interest (as well as identifying potential collinearity before modeling).\n",
    "\n",
    "*example*:\n",
    "```python\n",
    "sns.heatmap(df.corr())\n",
    "```\n",
    "\n",
    "Please take time to format your output, adding a title. Look through some of the additional arguments and options. (Axis labels aren't really necessary, as long as the title is informative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals = combined_all\n",
    "\n",
    "numericals.drop(columns =['state'], inplace=True)\n",
    "\n",
    "numericals.head();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(numericals.corr(), cmap=\"Blues\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom function to subplot histograms\n",
    "\n",
    "We have data for two tests for two years. We only have composite (and not subtest scores) for the 2018 ACT. We should write a function that will take the names of 2+ columns and subplot histograms. While you can use pandas plotting or Seaborn here, matplotlib gives you greater control over all aspects of your plots.\n",
    "\n",
    "[Helpful Link for Plotting Multiple Figures](https://matplotlib.org/users/pyplot_tutorial.html#working-with-multiple-figures-and-axes)\n",
    "\n",
    "Here's some starter code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subplot_histograms(dataframe, list_of_columns, list_of_titles, list_of_xlabels):\n",
    "    nrows = int(np.ceil(len(list_of_columns)/2)) # Makes sure you have enough rows\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=2) # You'll want to specify your figsize\n",
    "    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n",
    "    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n",
    "        ax[i].hist(dataframe[column]) # feel free to add more settings\n",
    "        plt.title(list_of_titles)\n",
    "        # Set titles, labels, etc here for each subplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and interpret histograms \n",
    "For each of the following:\n",
    "- Participation rates for SAT & ACT\n",
    "- Math scores for SAT & ACT\n",
    "- Reading/verbal scores for SAT & ACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = list(combined_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_all['Participation_x'].hist()\n",
    "\n",
    "# 2017 SAT participation\n",
    "subplot_histograms(combined_all,\n",
    "                   [\"'18_sat_partic\", \"'18_act_partic\"],\n",
    "                   ['2018 SAT Participation'],\n",
    "                   ['xlabel_a', 'xlabel_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all[['Participation_x', 'Participation_y']].hist()\n",
    "\n",
    "# 2017 ACT participation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all[\"'18_sat_partic\"].hist()\n",
    "\n",
    "\n",
    "# 2018 SAT Participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_all[\"'18_act_partic\"].hist()\n",
    "\n",
    "# 2018 ACT Participation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and interpret scatter plots\n",
    "\n",
    "For each of the following:\n",
    "- SAT vs. ACT math scores for 2017\n",
    "- SAT vs. ACT verbal/reading scores for 2017\n",
    "- SAT vs. ACT total/composite scores for 2017\n",
    "- Total scores for SAT 2017 vs. 2018\n",
    "- Composite scores for ACT 2017 vs. 2018\n",
    "\n",
    "Plot the two variables against each other using matplotlib or Seaborn\n",
    "\n",
    "Your plots should show:\n",
    "- Two clearly labeled axes\n",
    "- A proper title\n",
    "- Using colors and symbols that are clear and unmistakable\n",
    "\n",
    "**Feel free to write a custom function, and subplot if you'd like.** Functions save both time and space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working on this with Greenspan\n",
    "\n",
    "# Codeplt.figure(figsize=(10,6)) # (width, height)\n",
    "plt.title(\"SAT vs. ACT math scores for 2017\")\n",
    "sns.scatterplot(combined_all[\"Math_x\"], combined_all[\"Math_y\"])\n",
    "plt.xlabel(\"SAT Scores\")\n",
    "plt.ylabel(\"ACT Scores\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure.\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Generate a scatterplot of our data.\n",
    "plt.scatter(combined_all['Math_x'],\n",
    "            combined_all['Math_y'],\n",
    "            color = 'black')\n",
    "# Create a title.\n",
    "plt.title('2017 Math: SAT(x) vs ACT(y)', fontsize = 24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a figure.\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Generate a scatterplot of our data.\n",
    "plt.scatter(combined_all['Evidence-Based Reading and Writing'],\n",
    "            combined_all['Reading'],\n",
    "            color = 'black')\n",
    "# Create a title.\n",
    "plt.title('2017 Verbal: SAT(x) vs ACT(y)', fontsize = 24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure.\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Generate a scatterplot of our data.\n",
    "plt.scatter(combined_all['Total'],\n",
    "            combined_all['Composite'],\n",
    "            color = 'black')\n",
    "# Create a title.\n",
    "plt.title('2017 Total scores: SAT(x) vs ACT(y)', fontsize = 24);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and interpret boxplots\n",
    "\n",
    "For each numeric variable in the dataframe create a boxplot using Seaborn. Boxplots demonstrate central tendency and spread in variables. In a certain sense, these are somewhat redundant with histograms, but you may be better able to identify clear outliers or differences in IQR, etc.\n",
    "\n",
    "Multiple values can be plotted to a single boxplot as long as they are of the same relative scale (meaning they have similar min/max values).\n",
    "\n",
    "Each boxplot should:\n",
    "- Only include variables of a similar scale\n",
    "- Have clear labels for each variable\n",
    "- Have appropriate titles and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feel free to do additional plots below\n",
    "*(do research and choose your own chart types & variables)*\n",
    "\n",
    "Are there any additional trends or relationships you haven't explored? Was there something interesting you saw that you'd like to dive further into? It's likely that there are a few more plots you might want to generate to support your narrative and recommendations that you are building toward. **As always, make sure you're interpreting your plots as you go**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional): Using Tableau, create a choropleth map for each variable using a map of the US. \n",
    "\n",
    "Save this plot as an image file in an images directory, provide a relative path, and insert the image into notebook in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outside Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based upon your observations, choose **three** states that demonstrate interesting trends in their SAT and/or ACT participation rates. Spend some time doing outside research on state policies that might influence these rates, and summarize your findings below. **Feel free to go back and create new plots that highlight these states of interest**. If you bring in any outside tables or charts, make sure you are explicit about having borrowed them. If you quote any text, make sure that it renders as being quoted. (Make sure that you cite your sources -- check with you local instructor for citation preferences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your exploration of the data, what are you key takeaways and recommendations? Choose one state with a lower participation rate and provide a suggestion for how the College Board might increase participation amongst graduating seniors in this state. Are there additional data you desire that would better inform your investigations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Descriptive and Inferential Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarizing Distributions\n",
    "\n",
    "Above, we used pandas `describe` to provide quick summary statistics of our numeric columns. We also demonstrated many visual relationships.\n",
    "\n",
    "As data scientists, having a complete understanding of data is imperative prior to modeling.\n",
    "\n",
    "While we will continue to build our analytic tools, we know that measures of *central tendency*, *spread*, and *shape/skewness* provide a quick summary of distributions.\n",
    "\n",
    "For each variable in your data, summarize the underlying distributions (in words & statistics)\n",
    " - Be thorough in your verbal description of these distributions.\n",
    " - Be sure to back up these summaries with statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We generally assuming that data we sample from a population will be normally distributed. Do we observe this trend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does This Assumption Hold for:\n",
    "    - Math\n",
    "    - Reading\n",
    "    - Rates\n",
    "Explain your answers for each distribution and how you think this will affect estimates made from these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Limits of Data\n",
    "\n",
    "Suppose we only seek to understand the relationship between SAT and ACT participation rates in 2017. \n",
    "\n",
    "##### Does it make sense to conduct statistical inference given these data specifically? \n",
    "\n",
    "Why or why not?\n",
    "\n",
    "*(think about granularity, aggregation, the relationships between populations size & rates...consider the actually populations these data describe in answering this question)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Is it appropriate to compare *these* specific SAT and ACT math scores? \n",
    "\n",
    "Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Evaluation of Distributions \n",
    "\n",
    "**If you feel it's appropriate**, using methods we discussed in class, run hypothesis tests to compare variables of interest in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code:"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
